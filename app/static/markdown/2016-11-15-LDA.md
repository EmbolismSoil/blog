---
title: LDA算法推导
tags: 机器学习, LDA, 算法推导
subtitle:
grammar_cjkRuby: true
---

[TOC]

## 算法介绍
**线性判别分析(Liner Discriminat Analysis)** 是一种常见的二分类算法,其思想是: 找到一条直线,将所有的学习样例投影到样例上, 并且使得:

+ 所有同类别的样本在直线上的投影点都尽量聚集
+ 不同类别的样本在直线商投影的点都尽量拉开距离

下面是一个示例图：
![LDA算法示意图][1]

  [1]: http://my.csdn.net/uploads/201205/18/1337326287_3114.png

## 算法推导

假设有数据集`!$D\{x_i, y_i\}_{i=1}^m, y\in\{0,1\}$` ,`!$X_i$`为第i类数据的数据集, `!$\mu_i$`为其均值向量, `!$C_i$`为其协方差矩阵,`!$w$`为所寻找的直线向量
要让相同类别的样例投影点尽可能聚集，因为投影后的投影点与`!$w$`同方向，所以只需要考虑投影值即可，也就是可以把投影值`!$w^Tx_i$`看做一维数据，令相同类别的数据集合的投影方差尽可能小即可让投影点聚集．
```mathjax!
$
    C_i = \sum_{x \in X_i}(x - u_0)(x - u_0)^T
$
```
 投影后的方差为:
```mathjax!
$
    s_i = \sum_{x \in X_i}{(w^Tx - w^Tu_0)(w^Tx - w^Tu_0)^T} 
        \\ \ \ \ \ = \sum_{x \in X_i}{w^T(x - u_0)(x - u_0)w}
         \\ \ \ \ \ = w^TC_iw

$
```
考虑所有集合的情况下，就是**所有数据类别的点投影的方差之和最小**：
```mathjax!
$
   　令S_w = \sum_{i=0}^{+\infty}C_i则令方差和最小即为令w^TS_ww最小
$
```
要让不同类别的投影点尽可能远离，则在相同类别的投影点聚集成一簇之后，让相同类别的中心点，即**均值向量`!$u_i$`的投影`!$w^Tu_i$`相距尽可能大**即可：

```mathjax!
$
	||(w^Tu_{i=k} - w^Tu_{i \neq k})||_2^2 \\
	= (w^Tu_{i=k} - w^Tu_{i \neq k})(w^Tu_{i=k} - w^Tu_{i \neq k})^T \\ 
	= w^T(u_{i=k} - u_{i \neq k})(u_{i=k} - u_{i \neq k})^Tw \\
	令S_b = (u_{i=k} - u_{i \neq k})(u_{i=k} - u_{i \neq k})　\\
	则||(w^Tu_{i=k} - w^Tu_{i \neq k})||_2^2 = w^TS_bw
$
```
综合考虑：

+ 所有数据类别的点投影的方差之和最小
+ 均值向量的投影相距尽可能大

可以得到如下判别式：

```mathjax!
$
	J =  {w^TS_bw}/{w^TS_ww}
$
```
该式子就是LDA算法要最大化的目标.
可以看到，该式子的分子和分母都是二次型的形式，将矩阵`!$S_w$`看做是两个向量相乘而得`!$c_wr_w^T$`,同理`!$S_b$`看做`!$c_b r_b^T$`.则上面式子可以化为：
```mathjax!
$
	J =  {w^Tc_wr_w^Tw}/{w^Tc_br_b^Tw} \\
	\ \ \ = {(｜w｜｜c_w｜｜r_w｜｜w｜cos\alpha cos\beta)}/{(｜w｜｜c_b｜｜r_b｜｜w｜cos\theta cos\gamma)} \\
	\ \ \ = {(|c_w||r_w|cos\alpha cos\beta)} / {(｜c_b｜｜r_b|cos\theta cos\gamma)}
$
```
可见，上面式子的解跟`!$w$`的长度无关，只跟其角度有关，这意味着我们可以对`!$w$`的长度任意取值而不破坏原问题的解.故而我们可以假设`!$w^TS_ww=l$`，因为无论`!$w$`的角度`!$\alpha$`如何取值，总可以通过调整`!$||w||_2^2$`的值来使得`!$w^TS_ww=l$`成立．
由上面的讨论，可以知道原问题与下列问题等价：
```mathjax!
$
	min \ \ -w^TS_bw \\
	s.t. \ \ w^TS_ww=l
$
```
利用拉格朗日乘子法可以得到:
```mathjax!
$
	L(w, \lambda) = -w^TS_bw + \lambda(w^TS_ww - 1) \\
	\dfrac {\partial L}{\partial w} = -(S_b + S_b^T)w + \lambda(S_w + S_w^T)w = 0 \\　
	
$ 
```
显然有
```mathjax!
$
	{S_bw = \lambda S_ww \tag{1}}
$
```

在`!$(1)$`中`!$S_ww = \lambda(u_{i=k} - u_{i\neq k})(u_{i=k} - u_{i\neq k})^Tw$`其中`!$(u_{i=k} - u_{i\neq k})^Tw$`为常数，由于`!$w$`的长度可以任意取值,令`!$(u_{i=k} - u_{i\neq k})^Tw　＝１$`则有:
```mathjax!
$
	 S_bw = \lambda({u_{i=k}- u_{i\neq k}})  \tag{2}
$
```
将`!$(2)$`代回`!$(1)$`得到
```mathjax!
$
	w = S_w^{-1}(u_{i=k} - u_{i\neq k})  \tag{3}
$
```
上面的结果即为要求的`!$w$`，但注意到`!$S_w^{-1}$`只有在`!$S_w$`是非奇异阵的情况下才会存在.

