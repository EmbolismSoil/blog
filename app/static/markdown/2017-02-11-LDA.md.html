<p>[TOC]</p>
<h2>算法介绍</h2>
<p><strong>线性判别分析(Liner Discriminat Analysis)</strong> 是一种常见的二分类算法,其思想是: 找到一条直线,将所有的学习样例投影到样例上, 并且使得:</p>
<ul>
<li>所有同类别的样本在直线上的投影点都尽量聚集</li>
<li>不同类别的样本在直线商投影的点都尽量拉开距离</li>
</ul>
<p>下面是一个示例图：
<img alt="LDA算法示意图" src="http://my.csdn.net/uploads/201205/18/1337326287_3114.png" /></p>
<h2>算法推导</h2>
<p>假设有数据集<code>!$D\{x_i, y_i\}_{i=1}^m, y\in\{0,1\}$</code> ,<code>!$X_i$</code>为第i类数据的数据集, <code>!$\mu_i$</code>为其均值向量, <code>!$C_i$</code>为其协方差矩阵,<code>!$w$</code>为所寻找的直线向量
要让相同类别的样例投影点尽可能聚集，因为投影后的投影点与<code>!$w$</code>同方向，所以只需要考虑投影值即可，也就是可以把投影值<code>!$w^Tx_i$</code>看做一维数据，令相同类别的数据集合的投影方差尽可能小即可让投影点聚集．
<code>mathjax!
$
    C_i = \sum_{x \in X_i}(x - u_0)(x - u_0)^T
$</code>
 投影后的方差为:
```mathjax!
$
    s_i = \sum_{x \in X_i}{(w^Tx - w^Tu_0)(w^Tx - w^Tu_0)^T} 
        \ \ \ \ \ = \sum_{x \in X_i}{w^T(x - u_0)(x - u_0)w}
         \ \ \ \ \ = w^TC_iw</p>
<p>$
<code>考虑所有集合的情况下，就是**所有数据类别的点投影的方差之和最小**：</code>mathjax!
$
   　令S_w = \sum_{i=0}^{+\infty}C_i则令方差和最小即为令w^TS_ww最小
$
<code>``
要让不同类别的投影点尽可能远离，则在相同类别的投影点聚集成一簇之后，让相同类别的中心点，即**均值向量</code>!$u_i$<code>的投影</code>!$w^Tu_i$`相距尽可能大**即可：</p>
<p><code>mathjax!
$
    ||(w^Tu_{i=k} - w^Tu_{i \neq k})||_2^2 \\
    = (w^Tu_{i=k} - w^Tu_{i \neq k})(w^Tu_{i=k} - w^Tu_{i \neq k})^T \\ 
    = w^T(u_{i=k} - u_{i \neq k})(u_{i=k} - u_{i \neq k})^Tw \\
    令S_b = (u_{i=k} - u_{i \neq k})(u_{i=k} - u_{i \neq k})　\\
    则||(w^Tu_{i=k} - w^Tu_{i \neq k})||_2^2 = w^TS_bw
$</code>
综合考虑：</p>
<ul>
<li>所有数据类别的点投影的方差之和最小</li>
<li>均值向量的投影相距尽可能大</li>
</ul>
<p>可以得到如下判别式：</p>
<p><code>mathjax!
$
    J =  {w^TS_bw}/{w^TS_ww}
$</code>
该式子就是LDA算法要最大化的目标.
可以看到，该式子的分子和分母都是二次型的形式，将矩阵<code>!$S_w$</code>看做是两个向量相乘而得<code>!$c_wr_w^T$</code>,同理<code>!$S_b$</code>看做<code>!$c_b r_b^T$</code>.则上面式子可以化为：
<code>mathjax!
$
    J =  {w^Tc_wr_w^Tw}/{w^Tc_br_b^Tw} \\
    \ \ \ = {(｜w｜｜c_w｜｜r_w｜｜w｜cos\alpha cos\beta)}/{(｜w｜｜c_b｜｜r_b｜｜w｜cos\theta cos\gamma)} \\
    \ \ \ = {(|c_w||r_w|cos\alpha cos\beta)} / {(｜c_b｜｜r_b|cos\theta cos\gamma)}
$</code>
可见，上面式子的解跟<code>!$w$</code>的长度无关，只跟其角度有关，这意味着我们可以对<code>!$w$</code>的长度任意取值而不破坏原问题的解.故而我们可以假设<code>!$w^TS_ww=l$</code>，因为无论<code>!$w$</code>的角度<code>!$\alpha$</code>如何取值，总可以通过调整<code>!$||w||_2^2$</code>的值来使得<code>!$w^TS_ww=l$</code>成立．
由上面的讨论，可以知道原问题与下列问题等价：
<code>mathjax!
$
    min \ \ -w^TS_bw \\
    s.t. \ \ w^TS_ww=l
$</code>
利用拉格朗日乘子法可以得到:
```mathjax!
$
    L(w, \lambda) = -w^TS_bw + \lambda(w^TS_ww - 1) \
    \dfrac {\partial L}{\partial w} = -(S_b + S_b^T)w + \lambda(S_w + S_w^T)w = 0 \　</p>
<p>$ 
<code>显然有</code>mathjax!
$
    {S_bw = \lambda S_ww \tag{1}}
$
```</p>
<p>在<code>!$(1)$</code>中<code>!$S_ww = \lambda(u_{i=k} - u_{i\neq k})(u_{i=k} - u_{i\neq k})^Tw$</code>其中<code>!$(u_{i=k} - u_{i\neq k})^Tw$</code>为常数，由于<code>!$w$</code>的长度可以任意取值,令<code>!$(u_{i=k} - u_{i\neq k})^Tw　＝１$</code>则有:
<code>mathjax!
$
     S_bw = \lambda({u_{i=k}- u_{i\neq k}})  \tag{2}
$</code>
将<code>!$(2)$</code>代回<code>!$(1)$</code>得到
<code>mathjax!
$
    w = S_w^{-1}(u_{i=k} - u_{i\neq k})  \tag{3}
$</code>
上面的结果即为要求的<code>!$w$</code>，但注意到<code>!$S_w^{-1}$</code>只有在<code>!$S_w$</code>是非奇异阵的情况下才会存在.</p>